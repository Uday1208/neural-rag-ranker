# config/train_biencoder.yaml
# Training configuration for the MS MARCO bi-encoder ranker.

data:
  msmarco:
    hf_dataset_name: "microsoft/ms_marco"
    hf_config_name: "v1.1"
    train_split: "train"
    eval_split: "validation"

    # Subset sizes for quick experiments; you can scale up on AWS GPU.
    max_train_samples: 50000
    max_eval_samples: 5000

    # Tokenization lengths.
    max_query_length: 32
    max_passage_length: 128

model:
  name: "distilbert-base-uncased"
  pooling: "cls"          # "cls" or "mean"
  projection_dim: null    # e.g., 256 if you want an extra projection layer

training:
  batch_size: 16          # use 16 on CPU; increase on GPU
  num_epochs: 1           # start with 1; increase later
  learning_rate: 2.0e-5
  weight_decay: 0.01
  warmup_ratio: 0.1
  max_grad_norm: 1.0
  temperature: 0.05
  num_workers: 2          # DataLoader workers

  output_dir: "./checkpoints/biencoder"
  save_best: true
  log_every_steps: 100
